# SPARK properties.
# This values are injected from the properties sections in the pom.xml file.

# To set if it's local mode or yarn mode.
application.mode=${application.mode}

# true / false for Kerberos Security in Big Data Cluster
hadoop.security=${hadoop.security}
hadoop.security.principal=${hadoop.security.principal}
hadoop.security.keytab=${hadoop.security.keytab}
hadoop.security.authentication=${hadoop.security.authentication}
hbase.security.authentication=${hbase.security.authentication}
hbase.regionserver.kerberos.principal=${hbase.regionserver.kerberos.principal}

# spark connection to nodes and port with the username and the password.
username=${spark.master.username}
password=${spark.master.password}
host=${spark.master.host}
port=${spark.master.port}
zookeeper.quorum=${hbase.zookeeper.quorum}
zookeeper.clientport=${hbase.zookeeper.property.clientPort}


# Final name of the jar and the final jar path.
jar.name=${jar.name}
jar.dir=${jar.dir}

# Local folders to load scripts.
#script.dir=${script.dir}
#script.name=${script.name}

# Shell executer properties.
main.class.name=${main.class.name}
main.class.package=${main.class.package}
input.file.name=${input.file.name}
output.dir.name=${output.dir.name}

# Local folders to inputs, jars and libraries.
local.input.path=${local.input.path}
local.jars.path=${local.jars.path}
local.lib.path=${local.lib.path}

# Forecast local folders
forecast.local.log.path=${forecast.local.log.path}
forecast.local.sh.path=${forecast.local.sh.path}

# HDFS folders to inputs, jars and libraries.
hdfs.input.path=${hdfs.input.path}
hdfs.output.path=${hdfs.output.path}
hdfs.lib.path=${hdfs.lib.path}

# HDFS folder to use with Sqoop.
hdfs.sqoop.import.dir=${hdfs.sqoop.import.dir}
hdfs.output.sqoop.path=${hdfs.output.sqoop.path}
hdfs.input.sqoop.path=${hdfs.input.sqoop.path}